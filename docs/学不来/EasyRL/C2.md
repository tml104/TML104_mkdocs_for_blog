# C2

https://datawhalechina.github.io/easy-rl/#/chapter2/chapter2

## 2.1 马尔可夫过程

- 马尔可夫性质：随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态
- 马尔可夫链：离散时间的马尔可夫过程
  - 状态转移矩阵

## 2.2 马尔可夫奖励过程

- 马尔可夫奖励过程：马尔可夫链加上奖励函数
- 奖励函数：R
- 折扣因子
  - 可作为超参数学习
- 范围：一个回合的长度（每个回合最大的时间步数）
- 折扣回报：$G_t = r_{t+1}+\cdots+\gamma^{T-t-1}r_T$
- 状态价值函数：$V^t(s)=\mathbb{E}[G_t|s_t=s]$


- 贝尔曼方程

$$
  V(s)=\underbrace{R(s)}_{\text {即时奖励}}+\underbrace{\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s\right) V\left(s^{\prime}\right)}_{\text {未来奖励的折扣总和}}
$$

- 贝尔曼方程的矩阵形式:
  - 可以利用这种方法计算状态价值的解析解，但只适用于较小状态的情形
$$
  \begin{aligned}
    \boldsymbol{V} &= \boldsymbol{\boldsymbol{R}}+ \gamma \boldsymbol{P}\boldsymbol{V} \\
    \boldsymbol{I}\boldsymbol{V} &= \boldsymbol{R}+ \gamma \boldsymbol{P}\boldsymbol{V} \\
    (\boldsymbol{I}-\gamma \boldsymbol{P})\boldsymbol{V}&=\boldsymbol{R} \\
    \boldsymbol{V}&=(\boldsymbol{I}-\gamma \boldsymbol{P})^{-1}\boldsymbol{R}
    \end{aligned}
$$

  
- 计算马尔可夫奖励的迭代算法


## 2.3 马尔可夫决策过程

- 引入决策动作a
- 状态转移概率：取决于现在的状态和动作
- 奖励函数R：当前的状态以及采取的动作会决定智能体在当前可能得到的奖励多少
$$
p\left(s_{t+1} \mid s_{t}, a_{t}\right) =p\left(s_{t+1} \mid h_{t}, a_{t}\right)   \\
R\left(s_{t}=s, a_{t}=a\right)=\mathbb{E}\left[r_{t} \mid s_{t}=s, a_{t}=a\right]
$$

- 策略：$\pi(a\mid s) = p(a_t=a\mid s_t = s)$
- 策略下的状态转移：$P_{\pi}\left(s^{\prime} \mid s\right)=\sum_{a \in A} \pi(a \mid s) p\left(s^{\prime} \mid s, a\right)$
- 策略下的奖励函数：$r_{\pi}(s)=\sum_{a \in A} \pi(a \mid s) R(s, a)$

- 马尔可夫决策过程 与 马尔可夫（奖励）过程：前者可以由智能体决定未来的状态转移
- 