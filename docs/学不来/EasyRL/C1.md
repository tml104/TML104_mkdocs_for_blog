# C1

https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1?id=_114-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e5%ba%94%e7%94%a8

## 1.1 强化学习概述

- 强化学习
  - 智能体：agent
  - 环境：environment
  - 动作：action
  - 决策：decision

- 强化学习是非监督学习
  - 输入样本为序列，不满足独立同分布
  - 无标签，需要探索
  - 探索（exploration）和利用（exploitation）
  - 无监督者，只有奖励信号（reward signal）

- 特征
  - 延迟奖励
  - 试错与探索
  - 有时间关联的数据
  - 智能体的动作与采集到的数据关联，需要稳定智能体的动作

- 历史和应用

## 1.2 序列决策

- 奖励
- 序列决策
  - 历史：观测，动作，奖励的序列
  - 状态：对世界信息的完整描述
  - 观测：对世界信息的部分描述
  - 环境有自己的函数$s_t^e=f^e(H_t)$更新状态，智能体也有函数$s_t^a=f^a(H_t)$更新状态
  - 马尔可夫决策：$o_t=s_t^e=s_t^a$
  - 部分可观测马尔可夫决策：$(S,A,T,R,\omega,O,\gamma)$
    - S：状态空间（隐变量）
    - A：动作空间
    - T(s'|s,a)：状态转移概率
    - R：奖励函数
    - $\omega(o|s,a)$ ：观测概率


## 1.3 动作空间

- 离散动作空间
- 连续动作空间

## 1.4 强化学习智能体的组成成分与类型

- 策略：智能体会用策略来选取下一步的动作
  - 随机性策略：$\pi(a|s) = p(a_t=a|s_t=s)$，采取某个动作是随机的
  - 确定性策略：直接取pi函数中最大概率的动作。$a^{*}=\underset{a}{\arg \max} \pi(a \mid s)$
- 价值函数：用价值函数来对当前状态进行评估。价值函数用于评估智能体进 入某个状态后，可以对后面的奖励带来多大的影响。价值函数值越大，说明智能体进入这个状态越有利
  - 折扣因子 $\gamma$
  - V价值函数：$V_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1} \mid s_{t}=s\right], \text{对于所有的} s \in S$，在使用策略pi时的奖励
  - Q函数：$Q_{\pi}(s, a) \doteq \mathbb{E}_{\pi}\left[G_{t} \mid s_{t}=s, a_{t}=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} r_{t+k+1} \mid s_{t}=s, a_{t}=a\right]$，未来可以获得奖励的期望取决于当前的状态和当前的动作
- 模型：模型表示智能体对环境的状态进行理解，它决定了环境中世界的运行方式
  - 状态转移概率：$p_{s s^{\prime}}^{a}=p\left(s_{t+1}=s^{\prime} \mid s_{t}=s, a_{t}=a\right)$
  - 奖励函数：$R(s,a)=\mathbb{E}\left[r_{t+1} \mid s_{t}=s, a_{t}=a\right]$
  - 马尔可夫过程：策略、价值函数、模型


- 基于策略的强化学习：最大化奖励
  - 策略梯度（Policy Gradient，PG）
- 基于价值的强化学习：学习每个状态价值的不同以取得最佳策略
  - 仅离散环境下有好效果
  - Q学习（Q-learning）、 Sarsa
- 演员评论员智能体：二者的结合

- 有模型强化学习：学习状态的转移来采取动作
  - 需要对真实环境进行建模
  - 适用于数据匮乏的场景
  - 可以在虚拟世界中预测出将要发生的事，并采取对自己最有利的策略
- 免模型强化学习：学习价值函数和策略函数进行决策（而不是估计环境状态）
  - 数据驱动
  - 智能体只能一步一步地采取策略，等待真实环境的反馈

- 学习
- 规划：环境是已知的，智能体被告知了整个环境的运作规则的详细信息。智能体能够计算出一个完美的模型，并且在不需要与环境进行任何交互的时候进行计算
- 一个常用的强化学习问题解决思路是，先学习环境如何工作，也就是了解环境工作的方式，即学习得到一个模型，然后利用这个模型进行规划。

## 1.6 探索和利用

- 探索-利用窘境


